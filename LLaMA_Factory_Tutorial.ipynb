{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AssistMoli/LLaMA-Factory/blob/main/LLaMA_Factory_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA Factory Colab Tutorial\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [],
      "source": [
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/AssistMoli/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log in with Hugging Face account to upload model (Optional)"
      ],
      "metadata": {
        "id": "okkbTMoZCQNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "6OIm0O7oA5sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llmtuner import create_ui\n",
        "#\n",
        "# create_ui().queue().launch(share=True)"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing QA"
      ],
      "metadata": {
        "id": "sU2AEXLAtEBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_dataset = \"fintech_qa\""
      ],
      "metadata": {
        "id": "8cPxNVLctUWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(f'data/{your_dataset}.xlsx')\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dkMERsQ49Aen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from json import loads, dumps\n",
        "\n",
        "result = df.to_json(orient=\"records\", force_ascii=False)\n",
        "parsed = loads(result)\n",
        "dumps(parsed)"
      ],
      "metadata": {
        "id": "Q6kfgIoqpART"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(f\"data/{your_dataset}.json\", \"w\") as f:\n",
        "   json.dump(parsed, f, indent=4,ensure_ascii=False)"
      ],
      "metadata": {
        "id": "xc45Pqvk9KmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "WXh3bO4Lo7Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_model_name_or_path = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "your_template = \"qwen\"\n",
        "your_dataset = \"fintech_qa\"\n",
        "your_finetuning_type = \"lora\"\n",
        "your_output_dir = \"fintech_bot\"\n",
        "your_export_dir = \"fintech_bot_export\"\n",
        "overwrite = True\n",
        "your_learning_rate = 0.0001\n",
        "your_training_epoch = 50\n",
        "your_max_samples = 500"
      ],
      "metadata": {
        "id": "4xYDVDE2o6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line"
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import run_exp\n",
        "run_exp(dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  dataset= your_dataset ,\n",
        "  template=your_template,\n",
        "  finetuning_type= your_finetuning_type ,\n",
        "  lora_target=\"all\",\n",
        "  output_dir= your_output_dir ,\n",
        "  per_device_train_batch_size=4,\n",
        "  gradient_accumulation_steps=4,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=10,\n",
        "  save_steps=100,\n",
        "  learning_rate=your_learning_rate,\n",
        "  num_train_epochs= your_training_epoch,\n",
        "  max_samples=your_max_samples,\n",
        "  max_grad_norm=1.0,\n",
        "  fp16=True,\n",
        "  overwrite_output_dir = overwrite\n",
        "))\n",
        "\n"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import ChatModel\n",
        "chat_model = ChatModel(dict(\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  adapter_name_or_path= your_output_dir,\n",
        "  finetuning_type= your_finetuning_type,\n",
        "  template= your_template,\n",
        "))\n",
        "messages = []\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge LoRA weights"
      ],
      "metadata": {
        "id": "flmc2i3Z7Bl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import export_model\n",
        "export_model(dict(\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  adapter_name_or_path= your_output_dir,\n",
        "  finetuning_type= your_finetuning_type,\n",
        "  template= your_template,\n",
        "  export_dir= your_export_dir ,\n",
        "  # export_hub_model_id=\"your_hf_id/test_identity\",\n",
        "))"
      ],
      "metadata": {
        "id": "g0fVaJsj7GC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnES9PeuxOi9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}