{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AssistMoli/LLaMA-Factory/blob/main/%E3%80%8CLLaMA_Factory_Tutorial_ipynb%E3%80%8D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_model = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "\n",
        "input_text = \"被落石擊中！台鐵普悠瑪號445車次驚傳和平站出軌 大顆落石掉落軌道\"\n",
        "\n",
        "candidate_labels = [\"詐騙\"]\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Load zero-shot classification pipeline with GPT-2 model\n",
        "classifier = pipeline(\"zero-shot-classification\", model= your_model )\n",
        "\n",
        "# Define input text and candidate labels\n",
        "\n",
        "# Perform zero-shot classification\n",
        "classification_result = classifier(input_text, candidate_labels)\n",
        "\n",
        "# Display classification result\n",
        "print(\"Input Text:\", input_text)\n",
        "print(\"Predicted Label:\", classification_result[\"labels\"][0])\n",
        "print(\"Confidence Scores:\", classification_result[\"scores\"])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qjy9nyKenh6y",
        "outputId": "894ee128-8ff0-46ec-d1c8-6bd9fc4cd21e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B-Chat and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text: 被落石擊中！台鐵普悠瑪號445車次驚傳和平站出軌 大顆落石掉落軌道\n",
            "Predicted Label: 詐騙\n",
            "Confidence Scores: [0.04651827737689018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLaMA Factory Colab Tutorial\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [],
      "source": [
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/AssistMoli/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log in with Hugging Face account to upload model (Optional)"
      ],
      "metadata": {
        "id": "okkbTMoZCQNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "6OIm0O7oA5sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llmtuner import create_ui\n",
        "#\n",
        "# create_ui().queue().launch(share=True)"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing QA"
      ],
      "metadata": {
        "id": "sU2AEXLAtEBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_dataset = \"fintech_qa\""
      ],
      "metadata": {
        "id": "8cPxNVLctUWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(f'data/{your_dataset}.xlsx')\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dkMERsQ49Aen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from json import loads, dumps\n",
        "\n",
        "result = df.to_json(orient=\"records\", force_ascii=False)\n",
        "parsed = loads(result)\n",
        "dumps(parsed)"
      ],
      "metadata": {
        "id": "Q6kfgIoqpART"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(f\"data/{your_dataset}.json\", \"w\") as f:\n",
        "   json.dump(parsed, f, indent=4,ensure_ascii=False)"
      ],
      "metadata": {
        "id": "xc45Pqvk9KmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "WXh3bO4Lo7Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 不知所云的中文模型 XD\n",
        "your_model_name_or_path = \"hfl/chinese-llama-2-1.3b\"\n",
        "your_template = \"llama2\"\n",
        "\n",
        "your_dataset = \"fintech_qa\"\n",
        "\n",
        "your_finetuning_type = \"lora\"\n",
        "your_output_dir = \"fintech_bot\"\n",
        "your_export_dir = \"fintech_bot_export\"\n",
        "overwrite = True\n",
        "your_fp16 = False\n",
        "your_learning_rate = 0.000001\n",
        "your_training_epoch = 1\n",
        "your_max_samples = 500"
      ],
      "metadata": {
        "id": "4xYDVDE2o6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 好用，但是會讓你想買　A100　的模型\n",
        "your_model_name_or_path = \"FlagAlpha/Llama2-Chinese-7b-Chat\"\n",
        "your_template = \"llama2\"\n",
        "\n",
        "your_dataset = \"fintech_qa\"\n",
        "\n",
        "your_finetuning_type = \"lora\"\n",
        "your_output_dir = \"fintech_bot\"\n",
        "your_export_dir = \"fintech_bot_export\"\n",
        "overwrite = True\n",
        "your_fp16 = False\n",
        "your_learning_rate = 0.000001\n",
        "your_training_epoch = 1\n",
        "your_max_samples = 500"
      ],
      "metadata": {
        "id": "ZAFkH5iPHVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 原本作者 Qwen 的模型，還不錯用的輕量級中文模型，但簡體字居多，內容很多是中國大陸的內容\n",
        "your_model_name_or_path = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
        "your_template = \"qwen\"\n",
        "\n",
        "your_dataset = \"fintech_qa\"\n",
        "\n",
        "your_finetuning_type = \"lora\"\n",
        "your_output_dir = \"fintech_bot\"\n",
        "your_export_dir = \"fintech_bot_export\"\n",
        "overwrite = True\n",
        "your_fp16 = True\n",
        "your_learning_rate = 0.001\n",
        "your_training_epoch = 100\n",
        "your_max_samples = 5000\n",
        "\n"
      ],
      "metadata": {
        "id": "qjfAbmr5Bdhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line"
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import run_exp\n",
        "run_exp(dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  dataset= your_dataset ,\n",
        "  template=your_template,\n",
        "  finetuning_type= your_finetuning_type ,\n",
        "  lora_target=\"all\",\n",
        "  output_dir= your_output_dir ,\n",
        "  per_device_train_batch_size=4,\n",
        "  gradient_accumulation_steps=4,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=10,\n",
        "  save_steps=100,\n",
        "  learning_rate=your_learning_rate,\n",
        "  num_train_epochs= your_training_epoch,\n",
        "  max_samples=your_max_samples,\n",
        "  max_grad_norm=1.0,\n",
        "  fp16=your_fp16,\n",
        "  overwrite_output_dir = overwrite\n",
        "))\n",
        "\n"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import ChatModel\n",
        "chat_model = ChatModel(dict(\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  adapter_name_or_path= your_output_dir,\n",
        "  finetuning_type= your_finetuning_type,\n",
        "  template= your_template,\n",
        "))\n",
        "messages = []\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge LoRA weights"
      ],
      "metadata": {
        "id": "flmc2i3Z7Bl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llmtuner import export_model\n",
        "export_model(dict(\n",
        "  model_name_or_path= your_model_name_or_path,\n",
        "  adapter_name_or_path= your_output_dir,\n",
        "  finetuning_type= your_finetuning_type,\n",
        "  template= your_template,\n",
        "  export_dir= your_export_dir ,\n",
        "  # export_hub_model_id=\"your_hf_id/test_identity\",\n",
        "))"
      ],
      "metadata": {
        "id": "g0fVaJsj7GC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnES9PeuxOi9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}